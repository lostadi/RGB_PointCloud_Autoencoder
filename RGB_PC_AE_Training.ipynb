{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "119ea91e-9121-4515-a04f-ceabfceb51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Imports and Configuration--#\n",
    "import os#--import os module\n",
    "import numpy as np#--import numpy for numerical operations\n",
    "import torch#--import torch\n",
    "import torch.optim as optim#--import optim from torch\n",
    "import torch.nn.functional as F#--import functional from torch.nn\n",
    "from pytorch3d.loss import chamfer_distance#--import chamfer_distance loss function\n",
    "from RGB_model import PointCloudAE#--import PointCloudAE model from RGB_model\n",
    "from Dataloadersv2 import GetDataLoaders#--import GetDataLoaders function from Dataloadersv2\n",
    "import RGB_utils as utils#--import RGB_utils as utils\n",
    "import matplotlib.pyplot as plt#--import matplotlib for plotting\n",
    "import random#--import random module\n",
    "import math#--import math module\n",
    "import time#--import time module\n",
    "\n",
    "#--Parameters--#\n",
    "batch_size = 32#--set batch size\n",
    "output_dir = os.path.expanduser(\"~/meoutputsRGBpc/\")#--set output directory\n",
    "os.makedirs(output_dir, exist_ok=True)#--create output directory if it doesn't exist\n",
    "save_results = True#--flag to save results\n",
    "use_GPU = True#--flag to use GPU\n",
    "latent_size = 128#--set latent size for the autoencoder\n",
    "fixed_size = 1028#--set fixed point cloud size\n",
    "train_path = \"data/ModelNet10/alltrain.npy\"#--path to training data\n",
    "test_path = \"data/ModelNet10/alltest.npy\"#--path to test data\n",
    "train_processed = \"data/alltrain_adjusted.npy\"#--path to processed training data\n",
    "test_processed = \"data/alltest_adjusted.npy\"#--path to processed test data\n",
    "model_weights_path = 'General_rgb_1028pc_Encoder.pth'#--path to save/load model weights\n",
    "num_epochs = 500#--number of training epochs\n",
    "save_every = 20#--save model every 20 epochs\n",
    "plot_every = 50#--plot outputs every 50 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a603b0e-a915-4852-8449-7f4e1cd53b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Utility Functions--#\n",
    "def generate_random_function():\n",
    "    functions = [\n",
    "        lambda x, y, z: (x + y + z) % 1.0,#--sum modulo 1.0\n",
    "        lambda x, y, z: (math.sin(x) + math.cos(y) + z) % 1.0,#--sine and cosine modulation\n",
    "        lambda x, y, z: (x * y * z) % 1.0,#--product modulo 1.0\n",
    "        lambda x, y, z: ((x**2 + y**2 + z**2)**0.5) % 1.0,#--Euclidean norm modulo 1.0\n",
    "    ]\n",
    "    return random.choice(functions)#--select a random function\n",
    "\n",
    "def normalize_point_cloud(pc_array):\n",
    "    normalized_pc_array = []#--initialize list for normalized point clouds\n",
    "    for pc in pc_array:\n",
    "        spatial_coords = pc[:, :3]#--extract spatial coordinates\n",
    "        centroid = np.mean(spatial_coords, axis=0)#--compute centroid\n",
    "        spatial_coords -= centroid#--center the point cloud\n",
    "        d_max = np.max(np.linalg.norm(spatial_coords, axis=1))#--compute maximum distance\n",
    "        if d_max > 0:\n",
    "            spatial_coords /= d_max#--normalize to unit scale\n",
    "        pc[:, :3] = spatial_coords#--update spatial coordinates\n",
    "        normalized_pc_array.append(pc)#--add to normalized list\n",
    "    return np.array(normalized_pc_array)#--return as numpy array\n",
    "\n",
    "def assign_color_to_point_clouds(pc_array):\n",
    "    colored_pc_array = []#--initialize list for colored point clouds\n",
    "    for pc in pc_array:\n",
    "        r_func, g_func, b_func = generate_random_function(), generate_random_function(), generate_random_function()#--generate color functions\n",
    "        rgb_values = np.array([\n",
    "            [\n",
    "                max(0, min(1, r_func(x, y, z))),#--compute red channel\n",
    "                max(0, min(1, g_func(x, y, z))),#--compute green channel\n",
    "                max(0, min(1, b_func(x, y, z)))#--compute blue channel\n",
    "            ]\n",
    "            for x, y, z in pc#--apply to each point\n",
    "        ])#--create RGB array\n",
    "        colored_pc = np.hstack((pc, rgb_values))#--concatenate RGB to point cloud\n",
    "        colored_pc_array.append(colored_pc)#--add to colored list\n",
    "    return np.array(colored_pc_array)#--return as numpy array\n",
    "\n",
    "def adjust_point_clouds(pc_array, fixed_size=1028):\n",
    "    adjusted_pc_array = []#--initialize list for adjusted point clouds\n",
    "    for pc in pc_array:\n",
    "        num_points = pc.shape[0]#--number of points in current point cloud\n",
    "        if num_points < fixed_size:\n",
    "            indices = np.random.choice(num_points, fixed_size - num_points, replace=True)#--upsample indices\n",
    "            upsampled_points = pc[indices, :]#--upsampled points\n",
    "            adjusted_pc = np.vstack((pc, upsampled_points))#--append upsampled points\n",
    "        elif num_points > fixed_size:\n",
    "            indices = np.random.choice(num_points, fixed_size, replace=False)#--downsample indices\n",
    "            adjusted_pc = pc[indices, :]#--select subset of points\n",
    "        else:\n",
    "            adjusted_pc = pc#--no adjustment needed\n",
    "        adjusted_pc_array.append(adjusted_pc)#--add to adjusted list\n",
    "    return np.array(adjusted_pc_array)#--return as numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "431cacdb-2e25-49ad-951c-eae4316cff0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from data/alltrain_adjusted.npy\n",
      "Loading preprocessed data from data/alltest_adjusted.npy\n"
     ]
    }
   ],
   "source": [
    "#--Data Preprocessing--#\n",
    "def preprocess_point_clouds(input_path, output_path, fixed_size=1028):\n",
    "    if os.path.isfile(output_path):\n",
    "        print(f\"Loading preprocessed data from {output_path}\")#--inform about loading preprocessed data\n",
    "        return np.load(output_path, allow_pickle=True)#--load and return preprocessed data\n",
    "    else:\n",
    "        print(f\"Preprocessing data from {input_path}\")#--inform about preprocessing data\n",
    "        pc_array = np.load(input_path, allow_pickle=True)#--load raw point cloud data\n",
    "        colored_pc_array = assign_color_to_point_clouds(pc_array)#--assign colors to point clouds\n",
    "        adjusted_pc_array = adjust_point_clouds(colored_pc_array, fixed_size=fixed_size)#--adjust point cloud sizes\n",
    "        normalized_pc_array = normalize_point_cloud(adjusted_pc_array)#--normalize point clouds\n",
    "        np.save(output_path, normalized_pc_array)#--save preprocessed data\n",
    "        return normalized_pc_array#--return preprocessed data\n",
    "\n",
    "#--Preprocess Train and Test Sets--#\n",
    "adjusted_pc_train_array = preprocess_point_clouds(train_path, train_processed, fixed_size=fixed_size)#--preprocess training data\n",
    "adjusted_pc_test_array = preprocess_point_clouds(test_path, test_processed, fixed_size=fixed_size)#--preprocess test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfc0cb6-c302-47cd-8152-0287a0f41fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--DataLoaders--#\n",
    "train_loader, test_loader = GetDataLoaders(\n",
    "    train_source=train_processed,#--source for training data\n",
    "    test_source=test_processed,#--source for test data\n",
    "    batch_size=batch_size,#--batch size\n",
    "    shuffle=True,#--shuffle training data\n",
    "    num_workers=8,#--number of worker threads\n",
    ")#--initialize DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc4683e3-b6fb-4972-b2d2-65ed079d9ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved model weights.\n"
     ]
    }
   ],
   "source": [
    "#--Model Initialization--#\n",
    "point_size = fixed_size#--set point size\n",
    "net = PointCloudAE(point_size, latent_size)#--initialize PointCloudAE model\n",
    "\n",
    "if os.path.isfile(model_weights_path):\n",
    "    net.load_state_dict(torch.load(model_weights_path))#--load model weights if available\n",
    "    print(\"Loaded saved model weights.\")#--inform about loaded weights\n",
    "else:\n",
    "    print(\"No saved model weights found. Starting from scratch.\")#--inform about starting fresh\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_GPU and torch.cuda.is_available() else \"cpu\")#--set device to GPU if available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = torch.nn.DataParallel(net)#--use DataParallel if multiple GPUs\n",
    "net = net.to(device)#--move model to device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdcec868-47f3-47a1-97fa-760a93448f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Training Setup--#\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0006)#--initialize Adam optimizer with learning rate\n",
    "\n",
    "def combined_loss(pred, target):\n",
    "    pred_spatial, pred_color = pred[..., :3], pred[..., 3:]#--split predicted data into spatial and color\n",
    "    target_spatial, target_color = target[..., :3], target[..., 3:]#--split target data into spatial and color\n",
    "    spatial_loss, _ = chamfer_distance(pred_spatial, target_spatial)#--compute spatial chamfer loss\n",
    "    color_loss, _ = chamfer_distance(pred_color, target_color)#--compute color chamfer loss\n",
    "    return spatial_loss + color_loss#--return combined loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebdde1b0-7988-495d-946c-8eddf951d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Training and Testing Functions--#\n",
    "def train_epoch():\n",
    "    net.train()#--set model to training mode\n",
    "    epoch_loss = 0#--initialize epoch loss\n",
    "    for batch in train_loader:\n",
    "        data = batch[0] if isinstance(batch, (tuple, list)) else batch#--extract data from batch\n",
    "        data = data.to(device)#--move data to device\n",
    "        optimizer.zero_grad()#--zero the optimizer gradients\n",
    "        output = net(data.permute(0, 2, 1))#--forward pass (transpose for N, C, L format)\n",
    "        loss = combined_loss(output, data)#--compute loss\n",
    "        loss.backward()#--backpropagate\n",
    "        optimizer.step()#--update weights\n",
    "        epoch_loss += loss.item()#--accumulate loss\n",
    "    return epoch_loss / len(train_loader)#--return average loss\n",
    "\n",
    "def test_batch(data):\n",
    "    with torch.no_grad():#--disable gradient computation\n",
    "        data = data.to(device)#--move data to device\n",
    "        output = net(data.permute(0, 2, 1))#--forward pass\n",
    "        loss = combined_loss(output, data)#--compute loss\n",
    "    return loss.item(), output.cpu()#--return loss and output\n",
    "\n",
    "def test_epoch():\n",
    "    net.eval()#--set model to evaluation mode\n",
    "    epoch_loss = 0#--initialize epoch loss\n",
    "    with torch.no_grad():#--disable gradient computation\n",
    "        for data in test_loader:\n",
    "            loss, _ = test_batch(data)#--compute loss for batch\n",
    "            epoch_loss += loss#--accumulate loss\n",
    "    return epoch_loss / len(test_loader)#--return average loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d254562b-a0cf-4b0f-b79f-889d7f975186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, Loss:0.008933065481483937\n",
      "Iteration 40, Loss:0.008868347983807326\n",
      "Iteration 60, Loss:0.008871182285249234\n",
      "Iteration 80, Loss:0.008771081674844026\n",
      "Iteration 100, Loss:0.008819755848497153\n",
      "Iteration 120, Loss:0.008662331372499466\n",
      "Iteration 140, Loss:0.008604264426976442\n",
      "Iteration 160, Loss:0.008567607823759317\n",
      "Iteration 180, Loss:0.008598636712878943\n",
      "Iteration 200, Loss:0.008546384241431951\n",
      "Iteration 220, Loss:0.008451450351625681\n",
      "Iteration 240, Loss:0.008394523493945599\n",
      "Iteration 260, Loss:0.008425404407083989\n",
      "Iteration 280, Loss:0.008444365706294775\n",
      "Iteration 300, Loss:0.008317923326045275\n",
      "Iteration 320, Loss:0.008314042780548335\n",
      "Iteration 340, Loss:0.0082446807064116\n",
      "Iteration 360, Loss:0.008253087144345046\n",
      "Iteration 380, Loss:0.008232076652348042\n",
      "Iteration 400, Loss:0.008220853757113218\n",
      "Iteration 420, Loss:0.008151164323091507\n",
      "Iteration 440, Loss:0.008174640260636806\n",
      "Iteration 460, Loss:0.00815116873383522\n",
      "Iteration 480, Loss:0.008150010999292136\n",
      "Iteration 500, Loss:0.00808193475753069\n"
     ]
    }
   ],
   "source": [
    "#--Training Loop--#\n",
    "train_loss_list = []#--initialize list to store training losses\n",
    "test_loss_list = []#--initialize list to store test losses\n",
    "counter = 0#--initialize counter\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()#--record start time\n",
    "    \n",
    "    train_loss = train_epoch()#--train for one epoch and get average loss\n",
    "    train_loss_list.append(train_loss)#--append training loss\n",
    "    \n",
    "    test_loss = test_epoch()#--test on test set and get average loss\n",
    "    test_loss_list.append(test_loss)#--append test loss\n",
    "    \n",
    "    epoch_time = time.time() - start_time#--compute epoch duration\n",
    "    \n",
    "    log_str = f\"epoch {epoch} train loss:{train_loss} test loss:{test_loss} epoch time:{epoch_time}\\n\"#--create log string\n",
    "    \n",
    "    counter += 1#--increment counter\n",
    "    \n",
    "    if epoch % save_every == 0:\n",
    "        print(f\"Iteration {epoch}, Loss:{train_loss}\")#--print loss every save_every epochs\n",
    "        torch.save(net.state_dict(), model_weights_path)#--save model weights\n",
    "    \n",
    "    #--Plotting--#\n",
    "    plt.figure()#--create new figure\n",
    "    plt.plot(train_loss_list, label=\"Train\")#--plot training loss\n",
    "    plt.plot(test_loss_list, label=\"Test\")#--plot test loss\n",
    "    plt.legend()#--add legend\n",
    "    \n",
    "    if save_results:#--check if results should be saved\n",
    "        with open(os.path.join(output_dir, \"prints.txt\"), \"a\") as file:\n",
    "            file.write(log_str)#--append log to file\n",
    "        plt.savefig(os.path.join(output_dir, \"loss.png\"))#--save loss plot\n",
    "        plt.close()#--close plot\n",
    "        \n",
    "        if epoch % plot_every == 0:\n",
    "            test_samples = next(iter(test_loader))#--get a batch of test samples\n",
    "            loss, test_output = test_batch(test_samples)#--compute loss and output\n",
    "            utils.plotPCbatch(test_samples, test_output, show=False, save=True, name=os.path.join(output_dir, f\"epoch_{epoch}_test_set\"))#--plot and save test samples\n",
    "    else:\n",
    "        test_samples = next(iter(test_loader))#--get a batch of test samples\n",
    "        loss, test_output = test_batch(test_samples)#--compute loss and output\n",
    "        utils.plotPCbatch(test_samples, test_output, show=False, save=True, name=os.path.join(output_dir, f\"epoch_{epoch}_test_set\"))#--plot and save test samples\n",
    "        \n",
    "        train_samples = next(iter(train_loader))#--get a batch of training samples\n",
    "        loss, train_output = test_batch(train_samples)#--compute loss and output\n",
    "        utils.plotPCbatch(train_samples, train_output, show=False, save=True, name=os.path.join(output_dir, f\"epoch_{epoch}_train_set\"))#--plot and save training samples\n",
    "        \n",
    "        print(log_str)#--print log string\n",
    "        plt.show()#--display plot\n",
    "    \n",
    "    torch.cuda.empty_cache()#--clear GPU cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dd7f0c-56b8-4a64-8295-13483f7a7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss:0.018942401680196154\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "plotPCbatch() got an unexpected keyword argument 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m         utils\u001b[38;5;241m.\u001b[39mplotPCbatch(all_targets[i], all_outputs[i], show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;66;03m#--plot each sample\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#--Run Evaluation--#\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#--execute evaluation function\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m num_visuals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;66;03m#--number of samples to visualize\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_visuals):\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplotPCbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSample \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: plotPCbatch() got an unexpected keyword argument 'title'"
     ]
    }
   ],
   "source": [
    "#--Evaluation and Visualization--#\n",
    "def evaluate_model():\n",
    "    net.eval()#--set model to evaluation mode\n",
    "    total_loss = 0#--initialize total loss\n",
    "    all_outputs = []#--initialize list to store all outputs\n",
    "    all_targets = []#--initialize list to store all targets\n",
    "    \n",
    "    with torch.no_grad():#--disable gradient computation\n",
    "        for data in test_loader:\n",
    "            loss, output = test_batch(data)#--compute loss and output\n",
    "            total_loss += loss#--accumulate loss\n",
    "            all_outputs.append(output)#--store outputs\n",
    "            target = data[0] if isinstance(data, (tuple, list)) else data#--extract target data\n",
    "            all_targets.append(target.cpu())#--store targets\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)#--compute average loss\n",
    "    print(f\"Average Test Loss:{avg_loss}\")#--print average test loss\n",
    "    \n",
    "    #--Visualize Some Results--#\n",
    "    num_visuals = 5#--number of samples to visualize\n",
    "    for i in range(num_visuals):\n",
    "        utils.plotPCbatch(all_targets[i], all_outputs[i], show=True, save=False, title=f\"Sample {i+1}\")#--plot each sample\n",
    "\n",
    "#--Run Evaluation--#\n",
    "evaluate_model()#--execute evaluation function\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dAE_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
